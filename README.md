# HandsFree ðŸŽ™ï¸

**Voice-Controlled, Location-Aware Personal Agent**

> *"You're driving, hands on the wheel. You speak. Your phone acts instantly â€” sends your live GPS location, sets alarms, checks weather â€” all without touching the screen, all processed on-device in under 500ms."*

Built for the **Cactus x Google DeepMind â€” FunctionGemma Hackathon**

---

## What It Does

HandsFree is a voice-first personal agent built on the Cactus inference engine and Google's FunctionGemma model. Speak a command, and the system transcribes your audio on-device, detects location intent, grabs real GPS coordinates, intelligently routes between on-device and cloud inference, and executes function calls â€” all in a single seamless pipeline.

**Why it matters:** You can't type while driving, cycling, or cooking. HandsFree solves this with sub-500ms voice-to-action, mostly without internet.

---

## Architecture

```
User Speaks
    â”‚
    â–¼
cactus_transcribe (Whisper on-device) .............. ~120ms
    â”‚
    â–¼
Location Intent Detection (keyword matching) ....... ~0ms
    â”‚
    â–¼ [if location detected]
CoreLocation GPS + Reverse Geocode ................. ~50ms
    â”‚
    â–¼
Query Rewriting (inject address + Maps link)
    â”‚
    â–¼
Complexity Estimator (simple / medium / complex)
    â”‚
    â”œâ”€â”€[simple/medium]â”€â”€â–º FunctionGemma (on-device) .. ~80ms
    â”‚
    â””â”€â”€[complex]â”€â”€â”€â”€â”€â”€â”€â”€â–º Gemini Flash (cloud) ....... ~800ms
    â”‚
    â–¼
Action Executor â†’ Display Results + Pipeline Viz
```

**Smart Routing:** Simple single-tool commands stay blazing fast on-device. Complex multi-action requests gracefully fall back to Gemini in the cloud.

---

## Key Features

- **On-Device Transcription** â€” Whisper via Cactus, ~100-200ms, no internet required
- **Real GPS Location** â€” CoreLocation via pyobjc, reverse-geocoded to human-readable addresses
- **Hybrid Inference Routing** â€” Complexity estimator routes simple queries on-device, complex queries to cloud
- **Context Chains** â€” Pre-configured routines (e.g., "I've reached the office" triggers 4 actions in <100ms, no LLM needed)
- **Offline Mode** â€” ~80% of commands work without internet
- **Compare Mode** â€” Side-by-side view of FunctionGemma vs Gemini vs Hybrid routing with timing bars

---

## Example Scenarios

| Scenario | Command | Latency | Routing |
|---|---|---|---|
| Location Sharing | "Send my location to Mom" | ~280ms | 100% on-device |
| Context Chain | "I've reached the office" (triggers 4 actions) | ~350ms | On-device, skips LLM |
| Simple Query | "What's the weather in Tokyo?" | ~185ms | On-device |
| Complex Multi-Action | "Text Sarah I'm running late, check weather in NYC, set a 10 min timer" | ~930ms | Cloud (Gemini Flash) |
| Offline Mode | "Set an alarm for 6 AM and play morning playlist" | ~210ms | On-device, no internet |

---


## Setup

### Prerequisites

- macOS with Location Services enabled
- Cactus engine installed
- Gemini API key (for cloud fallback)

### Installation

```bash
# Download Whisper model
cactus download whisper-small

# Authenticate
cactus auth
export GEMINI_API_KEY="your-key-here"

# Install dependencies
pip install pyobjc-framework-CoreLocation streamlit audio-recorder-streamlit google-genai
```

### Enable Location Services

Go to **System Settings â†’ Privacy & Security â†’ Location Services** and allow your terminal app (Terminal or iTerm).

---

## Usage

### Run the Streamlit App

```bash
streamlit run app.py
```

### Run Benchmark

```bash
python benchmark.py
```

### Submit to Leaderboard

```bash
python submit.py --team "YourTeamName" --location "YourCity"
```

---

## Implementation Phases

| Phase | Task | Time |
|---|---|---|
| 1 | Foundation: verify build, download Whisper, install deps | 15 min |
| 2 | Location Service: CoreLocation + pyobjc + reverse geocode | 30 min |
| 3 | Voice Pipeline: orchestration, intent detection, GPS injection | 1 hr |
| 4 | Context Chains: pre-configured routines, chain matching | 30 min |
| 5 | Improve generate_hybrid: complexity routing, caching | 1 hr |
| 6 | Streamlit UI: mic, location sidebar, pipeline viz, compare mode | 1.5 hrs |
| 7 | Testing, benchmarking, submission | 30 min |
| **Total** | | **~5 hours** |

---

## Tech Stack

- **Cactus Engine** â€” On-device inference runtime
- **FunctionGemma (270M)** â€” On-device function calling model
- **Gemini Flash** â€” Cloud fallback for complex queries
- **Whisper (Small)** â€” On-device speech-to-text
- **CoreLocation (pyobjc)** â€” Native macOS GPS access
- **Streamlit** â€” Web UI with mic input and pipeline visualization

---

## Benchmark Scoring

- **F1 Accuracy** â€” 60% weight
- **Speed** â€” 15% weight (under 500ms = full marks)
- **On-Device Ratio** â€” 25% weight (more local = better)
- Difficulty weighting: easy 20%, medium 30%, hard 50%

---

*HandsFree â€” Built with Cactus Engine + Google FunctionGemma + Gemini Flash*
